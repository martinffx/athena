{
  "raw_user_story": "As a Claude Code user, I want to use the proxy to connect to OpenRouter with different model providers so that I can leverage OpenRouter's diverse model selection and competitive pricing while maintaining full compatibility with Claude Code's native API calling patterns.",
  "raw_criteria": "1. API Translation Accuracy: All supported Anthropic API requests are correctly translated to OpenAI format and responses are correctly translated back, maintaining semantic equivalence\n2. Streaming Support: Real-time streaming responses work with <100ms latency overhead, properly emitting Anthropic-formatted SSE events\n3. Tool Calling Functionality: Function/tool calling works end-to-end with proper definition transformation, tool use response mapping, and tool result handling\n4. Strict Compatibility Enforcement: Unsupported features (extended thinking, prompt caching, PDF input, batch API) are detected and rejected with clear error messages\n5. Claude Code Integration: Claude Code can successfully execute all core workflows (code generation, file operations, git operations, multi-step tasks) through the proxy",
  "raw_rules": "1. Strict Mode Operation: The proxy must operate in strict compatibility mode - reject any requests containing unsupported Anthropic features before forwarding to OpenRouter\n2. Model Name Transformation: All model names must be prefixed with \"anthropic/\" when forwarding to OpenRouter (e.g., \"claude-sonnet-4-20250514\" → \"anthropic/claude-sonnet-4-20250514\")\n3. System Prompt Positioning: System prompts from Anthropic's system parameter must always be converted to the first message with role=\"system\" in the OpenAI messages array\n4. Error Format Preservation: All errors from OpenRouter must be transformed to match Anthropic's error response format with proper type mapping\n5. Token Usage Mapping: Usage statistics must be renamed (prompt_tokens → input_tokens, completion_tokens → output_tokens) and total_tokens must be omitted\n6. Stream State Management: Streaming responses must maintain proper state machine transitions and emit events in the correct order (message_start → content_block_start → content_block_delta → content_block_stop → message_delta → message_stop)",
  "raw_scope": "**Included:**\n1. Bidirectional API translation (Anthropic ↔ OpenAI format)\n2. Non-streaming request/response handling\n3. Real-time streaming via Server-Sent Events (SSE)\n4. Tool/function calling translation (definitions, usage, results)\n5. Multi-modal content support (text + images)\n6. Error mapping and transformation\n7. Health check endpoint\n8. Configuration via environment variables and config files\n9. Request validation and unsupported feature detection\n10. Comprehensive logging and monitoring\n\n**Excluded:**\n1. Extended thinking support (Anthropic-specific feature)\n2. Prompt caching functionality\n3. Message Batches API\n4. PDF input support\n5. Request queuing and retry logic (Phase 2)\n6. Response caching (Phase 2)\n7. Multiple provider support and failover (Phase 2)\n8. Rate limiting functionality (Phase 2)\n9. Model routing and intelligent selection (Phase 3)\n10. Custom tool transformations (Phase 3)"
}
